{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import pandas as pd\n",
    "\n",
    "from twisted.internet import reactor\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import Request\n",
    "from scrapy.utils.log import configure_logging\n",
    "\n",
    "from w3lib.html import remove_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['parking', 'date', 'floor_plan', 'timestamp', 'floor_tiles', 'description', 'time', \n",
    "                           'area', 'publisher', 'title', 'currency', 'year', 'price', 'contact_name', 'insulated_glass',\n",
    "                          'bathrooms', 'parquet', 'contact_url', 'images', 'views', 'quarter', 'street', 'city',\n",
    "                          'url', 'wall_tiles', 'rooms', 'central_heating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.piata-az.ro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiataAZ(scrapy.Spider):\n",
    "    name = \"piata\"\n",
    "    allowed_domains = [\"piata-az.ro\"]\n",
    "#     start_urls = ['https://www.piata-az.ro/imobiliare/cluj-napoca']\n",
    "    start_urls = ['https://www.piata-az.ro/apartament-vanzare-3-camere-cluj-napoca-marasti-608096']\n",
    "    \n",
    "    \n",
    "    def parse(self, response):\n",
    "        self.logger.info('1. A response from %s has just arrived!', response.url)  \n",
    "        \n",
    "        continue_urls = response.xpath('//div[@class=\"announcement-wrapper announcement-space\"]/div//a/@href').extract()\n",
    "        continue_urls = [url + continue_url for continue_url in continue_urls if continue_url[0] ==\"/\"]\n",
    "        del continue_urls[::2]   # delete every second item, which is redundant\n",
    "        \n",
    "        continue_urls = ['https://www.piata-az.ro/apartament-vanzare-3-camere-cluj-napoca-marasti-608096']\n",
    "        for continue_url in continue_urls:\n",
    "            yield scrapy.Request(response.urljoin(continue_url), self.continue_parse)\n",
    "        \n",
    "    def continue_parse(self, response):\n",
    "        global df\n",
    "        self.logger.info('2. A response from %s has just arrived!', response.url) \n",
    "        url = response.url\n",
    "        \n",
    "        html_info_table = response.xpath('//div[@class=\"section-annoucement-details\"]/div/ul/li/div').extract()\n",
    "        \n",
    "        keys = [remove_tags(html_info_table[i]) for i in range(0, len(html_info_table), 2)]\n",
    "        values = [remove_tags(html_info_table[i]) for i in range(1, len(html_info_table), 2)]\n",
    "        info_table = dict(zip(keys, values))\n",
    "        \n",
    "        parking = info_table['parcare'] if 'parcare' in info_table else None\n",
    "        floor_tiles = info_table['gresie'] if 'gresie' in info_table else None\n",
    "        area = info_table['suprafata'] if 'suprafata' in info_table else None\n",
    "        publisher = info_table['Pers. fizica sau agentie'] if 'Pers. fizica sau agentie' in info_table else None\n",
    "        bathrooms = info_table['bai'] if 'bai' in info_table else None\n",
    "        parquet = info_table['parchet'] if 'parchet' in info_table else None\n",
    "        wall_tiles = info_table['faianta'] if 'faianta' in info_table else None\n",
    "        rooms = info_table['camere'] if 'camere' in info_table else None\n",
    "        central_heating = info_table['centrala termica'] if 'centrala termica' in info_table else None\n",
    "        street = info_table['strada'] if 'strada' in info_table and info_table['strada'] != \"-\" else None\n",
    "        insulated_glass = info_table['termopan'] if 'termopan' in info_table else None\n",
    "        floor_plan = info_table['compartimentare'] if 'compartimentare' in info_table else None\n",
    "        year = info_table['an constructie'] if 'an constructie' in info_table else None\n",
    "        \n",
    "        price = remove_tags(response.xpath('//div[@class=\"sidebar--details__top__price\"]/strong').extract()[0]).strip()\n",
    "        currency = remove_tags(response.xpath('//div[@class=\"sidebar--details__top__price\"]/b').extract()[0])\n",
    "        \n",
    "        city_neighborhood_details = response.xpath('//div[@class=\"sidebar--details__bottom\"]/ul/li/a/text()').extract()\n",
    "        city = city_neighborhood_details[0] if len(city_neighborhood_details) >= 1 else None\n",
    "        quarter = city_neighborhood_details[1] if len(city_neighborhood_details) == 2 else None\n",
    "        \n",
    "        contact = response.xpath('//div[@class=\"sidebar sidebar--contact\"]//a/@href').extract()\n",
    "        contact_url = contact[0] if len(contact) > 0 else None\n",
    "        contact = response.xpath('//div[@class=\"name-wrapper pull-left\"]/b/text()').extract()\n",
    "        contact_name = contact[0] if len(contact) > 0 else None\n",
    "        \n",
    "        date_time = response.xpath('//div[@class=\"announcement-detail__date-time pull-right\"]/span/text()').extract()\n",
    "        date = date_time[0]\n",
    "        time = date_time[1]\n",
    "        timestamp = (\"-\".join(date.split(\".\")[::-1])) + \" \" + time + \":00\"\n",
    "        \n",
    "        views = response.xpath('//div[@class=\"announcement-detail clearfix\"]/ul/li[@class=\"announcement-detail__list__views\"]/text()')\\\n",
    "                .extract()[0].strip().split(\" \")[0]\n",
    "        \n",
    "        title = response.xpath('//div[@class=\"col-md-12\"]/h1/text()').extract()[0]\n",
    "        description = response.xpath('//div[@class=\"offer-details__description\"]/text()').extract()[0].strip()\n",
    "        if \"href\" in description:\n",
    "            print(\"!FOUND\")\n",
    "        \n",
    "        image_urls = response.xpath('//a[@class=\"fancybox\"]/@href').extract()\n",
    "        images = \",\".join(image_urls) if len(image_urls) > 0 else None\n",
    "        \n",
    "        df = df.append({'parking' : parking, \\\n",
    "                   'date' : date, \\\n",
    "                   'floor_plan' : floor_plan, \\\n",
    "                   'timestamp' : timestamp, \\\n",
    "                   'floor_tiles' : floor_tiles, \\\n",
    "                   'description' : description, \\\n",
    "                   'time' : time, \\\n",
    "                   'area' : area, \\\n",
    "                   'publisher' : publisher, \\\n",
    "                   'title' : title, \\\n",
    "                   'currency' : currency, \\\n",
    "                   'year' : year, \\\n",
    "                   'price' : price, \\\n",
    "                   'contact_name' : contact_name, \\\n",
    "                   'insulated_glass' : insulated_glass, \\\n",
    "                   'bathrooms' : bathrooms, \\\n",
    "                   'parquet' : parquet, \\\n",
    "                   'contact_url' : contact_url, \\\n",
    "                   'images' : images, \\\n",
    "                   'views' : views, \\\n",
    "                   'quarter' : quarter, \\\n",
    "                   'street' : street, \\\n",
    "                   'city' : city, \\\n",
    "                   'url' : url, \\\n",
    "                   'wall_tiles' : wall_tiles, \\\n",
    "                   'rooms' : rooms, \\\n",
    "                   'central_heating' : central_heating}, ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-03 16:23:14 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2020-04-03 16:23:14 [scrapy.extensions.telnet] INFO: Telnet Password: 8b768dbfc6d7eafa\n",
      "2020-04-03 16:23:14 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-04-03 16:23:14 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-04-03 16:23:14 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-04-03 16:23:14 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-04-03 16:23:14 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-04-03 16:23:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-04-03 16:23:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-04-03 16:23:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.piata-az.ro/apartament-vanzare-3-camere-cluj-napoca-marasti-608096> (referer: None)\n",
      "2020-04-03 16:23:14 [piata] INFO: 1. A response from https://www.piata-az.ro/apartament-vanzare-3-camere-cluj-napoca-marasti-608096 has just arrived!\n",
      "2020-04-03 16:23:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.piata-az.ro/apartament-vanzare-3-camere-cluj-napoca-marasti-608096> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/scrapy/utils/defer.py\", line 117, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/scrapy/utils/python.py\", line 345, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/scrapy/utils/python.py\", line 345, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/scrapy/core/spidermw.py\", line 64, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/lib/python3.7/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/usr/local/lib/python3.7/site-packages/scrapy/core/spidermw.py\", line 64, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/lib/python3.7/site-packages/scrapy/spidermiddlewares/referer.py\", line 338, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/usr/local/lib/python3.7/site-packages/scrapy/core/spidermw.py\", line 64, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/lib/python3.7/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/usr/local/lib/python3.7/site-packages/scrapy/core/spidermw.py\", line 64, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/usr/local/lib/python3.7/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/usr/local/lib/python3.7/site-packages/scrapy/core/spidermw.py\", line 64, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"<ipython-input-4-a17e67ad459f>\", line 15, in parse\n",
      "    continue_urls = start_urls\n",
      "NameError: name 'start_urls' is not defined\n",
      "2020-04-03 16:23:14 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-04-03 16:23:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 269,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 13140,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.291131,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 4, 3, 13, 23, 14, 823473),\n",
      " 'log_count/DEBUG': 1,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 113856512,\n",
      " 'memusage/startup': 113856512,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'spider_exceptions/NameError': 1,\n",
      " 'start_time': datetime.datetime(2020, 4, 3, 13, 23, 14, 532342)}\n",
      "2020-04-03 16:23:14 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "configure_logging()\n",
    "runner = CrawlerRunner()\n",
    "runner.crawl(PiataAZ)\n",
    "# runner.crawl(MySpider2)\n",
    "d = runner.join()\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "\n",
    "reactor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parking</th>\n",
       "      <th>date</th>\n",
       "      <th>floor_plan</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>floor_tiles</th>\n",
       "      <th>description</th>\n",
       "      <th>time</th>\n",
       "      <th>area</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>contact_url</th>\n",
       "      <th>images</th>\n",
       "      <th>views</th>\n",
       "      <th>quarter</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>url</th>\n",
       "      <th>wall_tiles</th>\n",
       "      <th>rooms</th>\n",
       "      <th>central_heating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [parking, date, floor_plan, timestamp, floor_tiles, description, time, area, publisher, title, currency, year, price, contact_name, insulated_glass, bathrooms, parquet, contact_url, images, views, quarter, street, city, url, wall_tiles, rooms, central_heating]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 27 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
